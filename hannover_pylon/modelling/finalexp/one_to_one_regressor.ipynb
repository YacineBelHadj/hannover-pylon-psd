{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import settings\n",
    "from hannover_pylon.data import datamodules as dm\n",
    "from pathlib import Path\n",
    "from hannover_pylon.modelling.backbone.utils import FromBuffer , CutPSD, NormLayer\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from torch import nn\n",
    "freq_axis = np.linspace(0, 825.5, 8197)\n",
    "db_path = Path(settings.path.processed,'Welch(n_fft=16392, fs=1651, max_freq=825.5).db')\n",
    "columns= ['psd','level','direction']\n",
    "transform_func = [nn.Sequential(FromBuffer(),CutPSD(freq_axis=freq_axis,freq_range=(0,150)),NormLayer(min_val=-5.46,max_val=4.96))] + [nn.Identity()]*2\n",
    "query_key = f'''\n",
    "    SELECT id FROM data\n",
    "    WHERE date BETWEEN \"{settings.state.healthy_train.start}\" AND \"{settings.state.healthy_train.end}\"\n",
    "    AND corrupted = 0\n",
    "    AND sensor = \"accel\"\n",
    "'''\n",
    "data_loader = dm.PSDDataModule(db_path= db_path,table_name='data', columns=columns,transform_func=transform_func, query_key=query_key, batch_size=128, return_dict=True, cached=True,num_workers=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in data_loader.train_dataloader():\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from hannover_pylon.modelling.backbone import utils as ut\n",
    "\n",
    "def gaussian_nll_loss(mu, logvar, target):\n",
    "    \"\"\"\n",
    "    Compute the Gaussian negative log-likelihood loss for 1D regression.\n",
    "    For each sample, if the target is y, predicted mean is mu, and predicted log variance is s,\n",
    "    then the loss is:\n",
    "    \n",
    "        NLL = 0.5 * exp(-s) * (y - mu)^2 + 0.5 * s.\n",
    "    \n",
    "    Args:\n",
    "        mu (Tensor): Predicted location mean of shape (B, 1)\n",
    "        logvar (Tensor): Predicted log variance of shape (B, 1)\n",
    "        target (Tensor): Ground truth location of shape (B, 1)\n",
    "        \n",
    "    Returns:\n",
    "        Tensor: Averaged NLL loss.\n",
    "    \"\"\"\n",
    "    sq_error = (target - mu) ** 2  # shape: (B, 1)\n",
    "    loss = 0.5 * torch.exp(-logvar) * sq_error + 0.5 * logvar\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "class OneToOneAutoEncoderWithRegressorNLL(nn.Module):\n",
    "    def __init__(self, psd_length=1490, encoder_dims=[512, 128, 64], latent_dim=32):\n",
    "        \"\"\"\n",
    "        A one-to-one autoencoder that reconstructs a sensor's PSD (from key \"psd\") and also\n",
    "        predicts its 1D location with uncertainty.\n",
    "        \n",
    "        The encoder processes the PSD, and the decoder is symmetric to the encoder (i.e. the decoder's\n",
    "        hidden dimensions are the reverse of the encoder's). The location regressor outputs 2 values:\n",
    "          - The first is the predicted 1D location mean.\n",
    "          - The second is the predicted log variance.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Encoder: build layers using the provided encoder dimensions.\n",
    "        self.encoder = ut.build_layers(\n",
    "            hidden_dims=[psd_length] + encoder_dims,\n",
    "            activation_list='relu',\n",
    "            batch_norm=True,\n",
    "        )\n",
    "        self.latent_layer = nn.Linear(encoder_dims[-1], latent_dim)\n",
    "        \n",
    "        # Decoder: symmetric to the encoder.\n",
    "        decoder_dims = encoder_dims[::-1]\n",
    "        self.decoder = ut.build_layers(\n",
    "            hidden_dims=[latent_dim] + decoder_dims + [psd_length],\n",
    "            activation_list=['relu'] * (len(decoder_dims) + 1) + [None],\n",
    "            batch_norm=True,\n",
    "        )\n",
    "        \n",
    "        # Regressor: predicts the 1D sensor location and log variance.\n",
    "        self.location_regressor = nn.Sequential(\n",
    "            nn.Linear(latent_dim, latent_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(latent_dim // 2, 2)  # 2 outputs: [location_mean, location_logvar]\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (dict): Must contain a key \"psd\" with tensor of shape (B, psd_length).\n",
    "        \n",
    "        Returns:\n",
    "            dict: Contains:\n",
    "              - \"reconstruction\": reconstructed PSD (B, psd_length)\n",
    "              - \"latent\": latent embedding (B, latent_dim)\n",
    "              - \"location_mean\": predicted 1D location (B, 1)\n",
    "              - \"location_logvar\": predicted log variance (B, 1)\n",
    "        \"\"\"\n",
    "        psd = x[\"psd\"]  # (B, psd_length)\n",
    "        encoded = self.encoder(psd)\n",
    "        latent = self.latent_layer(encoded)\n",
    "        reconstruction = self.decoder(latent)\n",
    "        loc_out = self.location_regressor(latent)  # shape: (B, 2)\n",
    "        location_mean = loc_out[:, :1]              # (B, 1)\n",
    "        location_logvar = loc_out[:, 1:].unsqueeze(1) # (B, 1)\n",
    "        return {\n",
    "            \"reconstruction\": reconstruction,\n",
    "            \"latent\": latent,\n",
    "            \"location_mean\": location_mean,\n",
    "            \"location_logvar\": location_logvar\n",
    "        }\n",
    "\n",
    "\n",
    "class OneToOneTrainingModule(pl.LightningModule):\n",
    "    def __init__(self, psd_length=1490, encoder_dims=[512, 128, 64], latent_dim=32, lr=1e-3, location_loss_weight=1.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            location_loss_weight (float): Weight for the location regression (NLL) loss.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = OneToOneAutoEncoderWithRegressorNLL(psd_length=psd_length, encoder_dims=encoder_dims, latent_dim=latent_dim)\n",
    "        self.recon_loss_fn = nn.MSELoss()\n",
    "        self.lr = lr\n",
    "        self.location_loss_weight = location_loss_weight\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def _common_step(self, batch, batch_idx):\n",
    "        output = self(batch)\n",
    "        recon_loss = self.recon_loss_fn(output[\"reconstruction\"], batch[\"psd\"])\n",
    "        loc_loss = gaussian_nll_loss(output[\"location_mean\"], output[\"location_logvar\"], batch[\"location\"])\n",
    "        loss = recon_loss + self.location_loss_weight * loc_loss\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._common_step(batch, batch_idx)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._common_step(batch, batch_idx)\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
