{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def tensor_to_blob(data):\n",
    "    psd = data['data']['psd']  # Extract PSD tensor\n",
    "    psd = psd.numpy().astype(np.float32)  # Ensure 32-bit float format\n",
    "    return psd.tobytes()  # Convert NumPy array to binary (BLOB)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to combine data from multiple files into a single batch.\n",
    "    Each field is handled appropriately.\n",
    "    \"\"\"\n",
    "    # Initialize lists to hold batched data\n",
    "    file_paths = []\n",
    "    dates = []\n",
    "    corrupted_flags = []\n",
    "    time_series = []\n",
    "    fs_list = []\n",
    "    channel_names_list = []\n",
    "    channel_units_list = []\n",
    "    levels = []\n",
    "    directions = []\n",
    "    sensors = []\n",
    "\n",
    "    # Iterate over each data dictionary in the batch\n",
    "    for data in batch:\n",
    "        file_data = data['file_data']\n",
    "        time_series.extend((file_data['data']))  # Assuming data is converted to tensor for processing\n",
    "        number_of_samples = len(file_data['data'])\n",
    "        file_paths.append(data['file_path'])\n",
    "        dates.extend([data['date']]*number_of_samples)\n",
    "        corrupted_flags.extend([data['file_corrupted']]*number_of_samples)\n",
    "        \n",
    "        # Access the file_data dictionary for relevant fields\n",
    "        fs_list.append(file_data['fs'])\n",
    "        channel_names_list.extend(file_data['channel_names'])\n",
    "        channel_units_list.extend(file_data['channel_units'])\n",
    "        levels.extend(file_data['level'])\n",
    "        \n",
    "        # Handle directions and sensors, ensuring None values are preserved\n",
    "        directions.extend(file_data['direction'])\n",
    "        sensors.extend(file_data['sensor'])\n",
    "\n",
    "    # Convert lists to tensors where appropriate (e.g., psd_data)\n",
    "    time_series = torch.from_numpy(np.stack(time_series,dtype=np.float32))\n",
    "    data= {'time_series': time_series,\n",
    "           'channel_names': channel_names_list, 'channel_units': channel_units_list, \n",
    "           'levels': levels, 'directions': directions, 'sensors': sensors,\n",
    "           'corrupted_flags': corrupted_flags, 'dates': dates}\n",
    "\n",
    "    # Combine the data into a single dictionary for the batch\n",
    "    batch_data = {\n",
    "        'file_paths': file_paths,\n",
    "        'fs': fs_list,\n",
    "        'data': data \n",
    "    }\n",
    "\n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/owilab/Documents/YacineB/Code/hannover-pylon/.venv/lib/python3.10/site-packages/torchdata/datapipes/__init__.py:18: UserWarning: \n",
      "################################################################################\n",
      "WARNING!\n",
      "The 'datapipes', 'dataloader2' modules are deprecated and will be removed in a\n",
      "future torchdata release! Please see https://github.com/pytorch/data/issues/1196\n",
      "to learn more and leave feedback.\n",
      "################################################################################\n",
      "\n",
      "  deprecation_warning()\n"
     ]
    }
   ],
   "source": [
    "from config import settings\n",
    "from torchdata.datapipes import iter as it \n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from hannover_pylon.data_t import utils as data_utils\n",
    "from hannover_pylon.data_t import preprocessing as pp \n",
    "FS = 1651\n",
    "NFFT = 16392\n",
    "dp = it.FileLister(root= settings.path.raw,recursive=True,masks='*.mat')\n",
    "dp = dp.map(data_utils.process_file_path)\n",
    "dp_acc, dp_meta_met, dp_meta_struct, dp_corrupted = dp.demux(num_instances=4, \\\n",
    "        classifier_fn= data_utils.demux_data,buffer_size=70_000)\n",
    "dp_corrupted = dp_corrupted.map(data_utils.readfile)\n",
    "tracker = data_utils.TrackCorruptedFiles()\n",
    "dp_corrupted = dp_corrupted.map(tracker)\n",
    "list(dp_corrupted)\n",
    "tracker.setup()\n",
    "dp_acc = dp_acc.map(tracker)\n",
    "dp_acc = dp_acc.map(data_utils.readfile)\n",
    "dp_acc = dp_acc.map(data_utils.extract_channel_info)\n",
    "dp_acc = dp_acc.batch(30)\n",
    "dp_acc = dp_acc.collate(custom_collate_fn)\n",
    "dp_meta_met = dp_meta_met.map(data_utils.readfile)\n",
    "dp_meta_met = dp_meta_met.map(data_utils.extend_date)\n",
    "welch = pp.Welch(n_fft=NFFT, fs=FS)\n",
    "dp_acc = dp_acc.map(welch)\n",
    "ds = next(iter(dp_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'//home/owilab/Documents/YacineB/Code/hannover-pylon/data/processed'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlite3\n",
    "from pathlib import Path\n",
    "db_path = Path(settings.path.processed, 'data.db')\n",
    "conn = sqlite3.connect(db_path)\n",
    "c = conn.cursor()\n",
    "c.execute('''CREATE TABLE IF NOT EXISTS data\n",
    "             (id INTEGER PRIMARY KEY,\n",
    "             psd BLOB,\n",
    "             date TEXT,\n",
    "             level TEXT,\n",
    "            direction TEXT,\n",
    "            corrupted BOOLEAN,\n",
    "            sensor_type TEXT)''')\n",
    "conn.commit()\n",
    "insert_query = '''INSERT INTO data (psd, date, level, direction, corrupted, sensor_type) VALUES (?, ?, ?, ?, ?, ?)'''\n",
    "conn.execute('BEGIN TRANSACTION')\n",
    "for batch in dp_acc:\n",
    "\n",
    "    date_batch = batch['data']['dates']\n",
    "    psd_batch = batch['data']['log_psd']\n",
    "    level_batch = batch['data']['levels']\n",
    "    direction_batch = batch['data']['directions']\n",
    "    corrupted_batch = batch['data']['corrupted_flags']\n",
    "    sensor_batch = batch['data']['sensors']\n",
    "    psd_serialized = [psd.cpu().numpy().astype('float32').tobytes() for psd in psd_batch]\n",
    "    c.executemany(insert_query, list(zip(psd_serialized, date_batch, sensor_batch,level_batch, direction_batch, corrupted_batch)))\n",
    "    conn.commit()\n",
    "conn.execute('END TRANSACTION')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "DROP TABLE IF EXISTS \"reshaped\";\n",
    "\n",
    "CREATE TABLE \"reshaped\" AS\n",
    "SELECT\n",
    "  \"data\".\"date\" AS \"date\",\n",
    "  MAX(CASE WHEN sensor = 'accel' AND level = 1 AND direction = 'x' THEN psd END) AS \"psd_level1_x\",\n",
    "  MAX(CASE WHEN sensor = 'accel' AND level = 1 AND direction = 'y' THEN psd END) AS \"psd_level1_y\",\n",
    "  MAX(CASE WHEN sensor = 'accel' AND level = 2 AND direction = 'x' THEN psd END) AS \"psd_level2_x\",\n",
    "  MAX(CASE WHEN sensor = 'accel' AND level = 2 AND direction = 'y' THEN psd END) AS \"psd_level2_y\",\n",
    "  MAX(CASE WHEN sensor = 'accel' AND level = 3 AND direction = 'x' THEN psd END) AS \"psd_level3_x\",\n",
    "  MAX(CASE WHEN sensor = 'accel' AND level = 3 AND direction = 'y' THEN psd END) AS \"psd_level3_y\",\n",
    "  MAX(CASE WHEN sensor = 'accel' AND level = 4 AND direction = 'x' THEN psd END) AS \"psd_level4_x\",\n",
    "  MAX(CASE WHEN sensor = 'accel' AND level = 4 AND direction = 'y' THEN psd END) AS \"psd_level4_y\",\n",
    "  MAX(CASE WHEN sensor = 'accel' AND level = 5 AND direction = 'x' THEN psd END) AS \"psd_level5_x\",\n",
    "  MAX(CASE WHEN sensor = 'accel' AND level = 5 AND direction = 'y' THEN psd END) AS \"psd_level5_y\",\n",
    "  MAX(CASE WHEN sensor = 'accel' AND level = 6 AND direction = 'x' THEN psd END) AS \"psd_level6_x\",\n",
    "  MAX(CASE WHEN sensor = 'accel' AND level = 6 AND direction = 'y' THEN psd END) AS \"psd_level6_y\",\n",
    "  MAX(CASE WHEN sensor = 'accel' AND level = 7 AND direction = 'x' THEN psd END) AS \"psd_level7_x\",\n",
    "  MAX(CASE WHEN sensor = 'accel' AND level = 7 AND direction = 'y' THEN psd END) AS \"psd_level7_y\",\n",
    "  MAX(CASE WHEN sensor = 'accel' AND level = 8 AND direction = 'x' THEN psd END) AS \"psd_level8_x\",\n",
    "  MAX(CASE WHEN sensor = 'accel' AND level = 8 AND direction = 'y' THEN psd END) AS \"psd_level8_y\",\n",
    "  MAX(CASE WHEN sensor = 'accel' AND level = 9 AND direction = 'x' THEN psd END) AS \"psd_level9_x\",\n",
    "  MAX(CASE WHEN sensor = 'accel' AND level = 9 AND direction = 'y' THEN psd END) AS \"psd_level9_y\"\n",
    "FROM \"data\"\n",
    "GROUP BY \"data\".\"date\";\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
